
# Informe del Proyecto EDA: Procesamiento y Ordenamiento (Parte 1)

## 1\. Implementaci칩n Java (Single-Core): Preparaci칩n de Datos

El primer componente del proyecto consiste en procesar un dataset de gran tama침o (`yelp_database.csv`) utilizando Java en un solo n칰cleo. El objetivo no es solo leer los datos, sino tambi칠n limpiarlos, transformarlos y prepararlos para la fase de an치lisis de algoritmos.

### Descripci칩n de la Clase `Restaurante`

Para modelar los datos en el programa de an치lisis (`AnalisisOrdenamiento.java`), se defini칩 una clase `Restaurante`. Esta clase act칰a como un Objeto Simple de Java (POJO) o "molde" que encapsula los atributos de cada registro que nos interesa:

```java
public class Restaurante {
    String nombreREstaurante;
    double rating; 
    int numeroResenas;
    double puntuacionTotal; // Almacena el resultado de la f칩rmula

    // Constructor para inicializar el objeto
    public Restaurante(String nombre, double rating, int resenas, double puntuacion) {
        this.nombreREstaurante = nombre;
        this.rating = rating;
        this.numeroResenas = resenas;
        this.puntuacionTotal = puntuacion;
    }

    // M칠todo de utilidad para depuraci칩n
    @Override
    public String toString() {
        return String.format("Restaurante[%s, Puntuacion: %.4f]", 
                             nombreREstaurante, puntuacionTotal);
    }
}
```

-----

## 2\. Desaf칤os de Implementaci칩n y Soluciones

Procesar un dataset "sucio" de 1 mill칩n de filas present칩 varios desaf칤os t칠cnicos que requirieron soluciones espec칤ficas para garantizar la eficiencia y la integridad de los datos.

### Optimizaci칩n de Memoria: Enfoque de Streaming (I/O)

Al procesar el archivo original "sucio" (`limpiarDatos.java`), se descart칩 el enfoque tradicional de leer todas las l칤neas en un `ArrayList`.

  * **Problema:** Cargar 1 mill칩n de objetos Java en la memoria RAM (Heap) tiene un consumo muy alto. Cada objeto `String` y `Double`, sumado al *overhead* del propio objeto `Restaurante`, podr칤a consumir varios gigabytes de RAM, llevando a un error `java.lang.OutOfMemoryError`.
  * **Soluci칩n (Enfoque 칍ptimo):** Se implement칩 un enfoque de "streaming" (flujo de datos). El programa utiliza un `BufferedReader` para leer una sola l칤nea a la vez y un `BufferedWriter` para escribir la l칤nea procesada de inmediato.
      * La memoria utilizada es m칤nima y constante, ya que solo existe una l칤nea en la RAM en un momento dado.
      * Esto nos permite procesar archivos de tama침o virtualmente ilimitado (1 mill칩n o 1 bill칩n de filas) sin agotar los recursos del sistema.

### Manejo de Datos Corruptos: La Estrategia del Doble `try-catch`

El dataset original estaba "sucio" y el m칠todo `linea.split(",")` es muy fr치gil.

  * **Problema:** Se encontraron dos tipos de corrupci칩n que lanzaban `NumberFormatException`:
    1.  **Datos No Num칠ricos:** Campos que deb칤an ser n칰meros conten칤an texto (ej. `" and Catering""`).
    2.  **Desplazamiento de Columnas:** Comas (`.`) dentro del nombre de un restaurante (ej. `"Zaxby's, Chicken & Wings"`) enga침aban al `split()`, provocando que el `Rating` (ej. `"3.5"`) se leyera en la columna de `NumberReview`, lo que causaba un error al intentar `Integer.parseInt("3.5")`.
  * **Soluci칩n:** Se implement칩 una estrategia de "doble `try-catch`" para hacer el programa robusto.
    1.  **`try-catch` en `calcularRatingPromedio`:** Permite que el c치lculo del promedio `C` ignore las l칤neas corruptas sin detener el programa.
    2.  **`try-catch` en `main`:** Protege el bucle principal de escritura. Si una l칤nea est치 desplazada o corrupta, se captura la excepci칩n, se imprime un error en la consola y el programa contin칰a con la siguiente l칤nea, asegurando que solo los datos v치lidos se escriban en el archivo limpio.

### Desaf칤o de Configuraci칩n Regional (Locale) y `.trim()`

  * **Problema 1 (Locale):** El programa fallaba al leer n칰meros v치lidos como `"4.5"`. Esto se deb칤a a un conflicto de Configuraci칩n Regional (Locale): el `.csv` usaba puntos (`.`) como separador decimal, pero el sistema operativo (y Java por defecto) esperaba comas (`,`). Ir칩nicamente, al escribir los datos, `String.format()` generaba comas (ej. `"2,8290"`), corrompiendo el `.csv` de salida.

  * **Soluci칩n 1:** Se forz칩 a Java a usar el est치ndar de EE.UU. (que usa puntos) al inicio del `main`:

    ```java
    Locale.setDefault(Locale.US);
    ```

  * **Problema 2 (Datos "sucios"):** Incluso con el Locale corregido, el programa fallaba con entradas como `""4.5""` (con comillas) o `" 12 "` (con espacios).

  * **Soluci칩n 2:** Se "sanitizaron" (limpiaron) los strings antes de convertirlos, usando dos m칠todos encadenados:

    ```java
    // Ej: partes[5] es "\" 4.5 \""
    String ratingLimpio = partes[5].replace("\"", "").trim();
    // 1. .replace("\"", "") -> " 4.5 " (quita comillas)
    // 2. .trim()           -> "4.5" (quita espacios)
    double R = Double.parseDouble(ratingLimpio); // Funciona
    ```

-----

## 3\. L칩gica del Proyecto: La F칩rmula de Confianza

Para que la ordenaci칩n fuera significativa, no bastaba con usar el `Rating` simple. Un restaurante con 1 rese침a de 5 estrellas no es "mejor" que uno con 1000 rese침as de 4.9 estrellas.

Se implement칩 una **f칩rmula de "Puntuaci칩n de Confianza"** (un promedio ponderado de estilo Bayesiano) para crear un ranking m치s justo.

**La F칩rmula:**
$$Puntuaci칩n = \left( \frac{v}{v+m} \times R \right) + \left( \frac{m}{v+m} \times C \right)$$

**Descripci칩n Detallada de los Componentes:**

  * **`R`** = `Rating`. Es la calificaci칩n individual del restaurante (ej. 4.9).
  * **`v`** = `NumberReview`. Es el n칰mero de rese침as (votos) de *ese* restaurante (ej. 1000).
  * **`C`** = Rating Promedio Global. Es la calificaci칩n promedio de *todo* el dataset (ej. `2.7821...`). Este valor lo calculamos en la "Pasada 1" de nuestro programa `limpiarDatos.java`.
  * **`m`** = M칤nimo de Rese침as. Es una constante que nosotros definimos (ej. `100`). Act칰a como una "perilla de confianza": es el n칰mero de rese침as que consideramos necesario para que un rating empiece a ser estad칤sticamente fiable.

**C칩mo Funciona la L칩gica:**

La f칩rmula es una "lucha de poder" entre el rating individual (`R`) y el promedio global (`C`). El n칰mero de rese침as (`v`) decide qui칠n gana.

  * **Caso 1: Restaurante Popular (muchas rese침as, `v >> m`)**

      * `v = 1000`, `R = 4.9`.
      * La primera parte de la f칩rmula $\left( \frac{v}{v+m} \right)$ ser치 $\left( \frac{1000}{1100} \right) \approx 0.91$ (91%).
      * La segunda parte $\left( \frac{m}{v+m} \right)$ ser치 $\left( \frac{100}{1100} \right) \approx 0.09$ (9%).
      * `Puntuaci칩n = (0.91 * 4.9) + (0.09 * 2.78)`
      * **Resultado:** La puntuaci칩n final ser치 muy cercana a su rating real de 4.9. **L칩gica:** "Confiamos en este rating porque tiene muchas rese침as".

  * **Caso 2: Restaurante Nuevo (pocas rese침as, `v << m`)**

      * `v = 1`, `R = 5.0`.
      * La primera parte $\left( \frac{v}{v+m} \right)$ ser치 $\left( \frac{1}{101} \right) \approx 0.01$ (1%).
      * La segunda parte $\left( \frac{m}{v+m} \right)$ ser치 $\left( \frac{100}{101} \right) \approx 0.99$ (99%).
      * `Puntuaci칩n = (0.01 * 5.0) + (0.99 * 2.78)`
      * **Resultado:** La puntuaci칩n final ser치 "arrastrada" fuertemente hacia el promedio `C` de 2.78. **L칩gica:** "No confiamos en este 5.0; es estad칤sticamente irrelevante. Lo trataremos como si fuera 'promedio' hasta que tenga m치s rese침as".

Esta `PuntuacionConfianza` es la columna que usaremos para nuestro an치lisis de algoritmos de ordenamiento.

-----`
# Multiprocesamiento y Ordenamiento (Parte 2)

## 1. Implementaci칩n en Python (Single-Core): Preparaci칩n de Datos

La segunda parte del proyecto consiste en replicar la l칩gica de procesamiento de datos de Java a Python. El objetivo es crear una base de datos limpia id칠ntica que sirva como punto de entrada para el an치lisis de algoritmos de ordenamiento.

Para esta implementaci칩n, se opt칩 por utilizar la biblioteca **Pandas**, un est치ndar de facto en el ecosistema de Python para la manipulaci칩n y an치lisis de datos.

---

### Enfoque de Implementaci칩n: Pandas para Procesamiento Vectorizado

A diferencia del enfoque de "streaming" (l칤nea por l칤nea) implementado en Java para optimizar el uso de memoria RAM, el enfoque de Python utiliza la biblioteca Pandas, que carga el dataset completo en memoria en una estructura de datos llamada `DataFrame`.

- **Diferencia Clave:** Mientras que la soluci칩n de Java (`BufferedReader`) se dise침칩 para un consumo de memoria m칤nimo y constante (evitando `OutOfMemoryError`), la soluci칩n de Python (`pd.read_csv`) carga todo el mill칩n de filas en la RAM.  
- **Justificaci칩n:** Se asume un entorno con suficiente RAM. A cambio de un mayor consumo de memoria, Pandas ofrece una API de "procesamiento vectorizado" que simplifica enormemente las operaciones de limpieza y transformaci칩n de datos, permitiendo aplicar cambios a columnas enteras de una sola vez.


### Manejo de Datos "Sucios" con Pandas

Los mismos desaf칤os de datos corruptos encontrados en la Parte 1 fueron resueltos usando funciones optimizadas de Pandas, que reemplazan los bucles `try-catch` manuales de Java.

- **Problema (Java):** Datos con comillas (`""4.5""`) y espacios (`" 12 "`) que requer칤an `.replace("\"", "").trim()` por cada l칤nea.

- **Soluci칩n (Python):** Se aplic칩 una "limpieza vectorizada" a todas las columnas de texto simult치neamente:

```python
# Limpia comillas y espacios en todas las columnas de texto
datos[col] = datos[col].astype(str).str.replace('"', '', regex=False).str.strip()
```

- **Problema (Java)**
Datos no num칠ricos (`" and Catering"`) y columnas desplazadas (`"3.5"` en la columna de tipo *Integer*) requer칤an un **doble `try-catch`** para evitar el `NumberFormatException`.

- **Soluci칩n (Python**)
**Pandas** maneja esto de forma m치s robusta.  
Se utiliza `pd.to_numeric` con el argumento `errors='coerce'`.  
Esta funci칩n intenta convertir la columna, y cualquier valor que falle (como `" and Catering"`) se transforma autom치ticamente en `NaN` (*Not a Number*).

```python
# Convierte a n칰mero, los errores se marcan como NaN
datos['rating'] = pd.to_numeric(datos['rating'], errors='coerce')
datos['num_reviews'] = pd.to_numeric(datos['num_reviews'], errors='coerce')

# Elimina todas las filas que fallaron la conversi칩n
datos = datos.dropna(subset=['rating', 'num_reviews'])
```

Este enfoque elimina eficazmente todas las l칤neas corruptas o desplazadas sin necesidad de bloques `try-except` expl칤citos por cada fila.

### L칩gica del Proyecto: Replicando la F칩rmula en Pandas

La **f칩rmula de "Puntuaci칩n de Confianza"** sigue siendo el n칰cleo del proyecto.

### F칩rmula 

\[
Puntuaci칩n = \left(\frac{v}{v+m} \times R\right) + \left(\frac{m}{v+m} \times C\right)
\]


### C치lculo de C (Promedio Global)

En lugar de una "Pasada 1" manual, **Pandas** calcula el promedio `C` de la columna `rating` (ya limpia) con un simple m칠todo:

```python
C = datos['rating'].mean()
```

### Aplicaci칩n de la F칩rmula

La f칩rmula se aplica a cada fila del **DataFrame** usando el m칠todo `.apply(axis=1)`.  
Este m칠todo itera sobre cada fila (`row`) y ejecuta la funci칩n `calcular_puntuacion_row`,  
la cual contiene la **l칩gica bayesiana id칠ntica** a la versi칩n implementada en **Java**.

```python
def calcular_puntuacion_row(row):
    R = row['rating']
    v = row['num_reviews']
    if (v + m) != 0:
        puntuacion = (v / (v + m)) * R + (m / (v + m)) * C
    else:
        puntuacion = 0
    return round(puntuacion, 2)

# Crea la nueva columna 'puntuacion' aplicando la funci칩n
datos['puntuacion'] = datos.apply(calcular_puntuacion_row, axis=1)
```
El **DataFrame resultante**, que contiene las columnas:

- `Organization`
- `Rating`
- `NumberReview`
- `PuntuacionConfianza`

se guarda en el archivo `datos_limpios.csv`.  
Este archivo sirve como **entrada estandarizada** para la fase de **an치lisis de algoritmos**.

---

## 2 An치lisis de Algoritmos de Ordenamiento (Single-Core)

Esta fase del proyecto (correspondiente a **`AnalisisOrdenamiento.py`**) se centra en **cargar los datos limpios** y **medir el rendimiento** de diferentes algoritmos de ordenamiento.

---

### Carga de Datos y Estructura

El script `datos_limpios.csv` se carga nuevamente usando **Pandas**, pero para la fase de ordenamiento se transforma la estructura de datos.

- `datos.to_dict('records')`: El DataFrame de Pandas se convierte en una lista est치ndar de diccionarios de Python.

- **Motivo**: Los algoritmos de ordenamiento cl치sicos (como Bubble Sort y QuickSort) est치n dise침ados para operar sobre listas en memoria, no sobre DataFrames.
Cada diccionario de la lista representa un restaurante, por ejemplo:
 ```python
 {'Organization': 'Taco Bell', 'Rating': 3.5, 'NumReviews': 120, 'Puntuacion': 4.2}
```
### Implementaci칩n y Comparativa de Algoritmos

El script est치 dise침ado para **comparar el rendimiento** de diferentes algoritmos al ordenar la lista de diccionarios por la variable `puntuacion_total` (calculada en el script).

---

### 游댳 Bubble Sort (`buble_sort`)

Se implementa este **algoritmo cl치sico**.  
Aunque es conocido por su ineficiencia en *datasets* grandes (complejidad \( O(n^2) \)),  
sirve como una **l칤nea base fundamental** para la comparaci칩n.

---

### 游댳 QuickSort (`platzhalter`)

El men칰 incluye una opci칩n para **QuickSort**, un algoritmo mucho m치s eficiente del tipo **"Divide y Vencer치s"**,  
con una **complejidad promedio** de \( O(n \log n) \).  
Sirve como una **comparativa de rendimiento m치s realista** frente a Bubble Sort.

---

### Medici칩n de Rendimiento

Para **cuantificar el rendimiento**, se utiliza la biblioteca `time`.  
Se registra:

- El tiempo inicial (`time_inicio`) justo antes de llamar a la funci칩n de ordenamiento  
  (por ejemplo, `buble_sort`), y  
- El tiempo final (`time_fin`) inmediatamente despu칠s de completarla.

---

Esto permite **aislar y medir exclusivamente el tiempo de c칩mputo** (*CPU-Bound*)  
del algoritmo de ordenamiento, que es la **m칠trica clave** para el an치lisis de la **Parte 2**.

---

### Resultado Final

El archivo resultante `datos_ordenados.csv` incluye la **posici칩n (ranking)** final de cada restaurante.




